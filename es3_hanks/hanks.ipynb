{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scegliere un verbo transitivo --> **KILL**\n",
    "- Trovare un corpus con > 1000 frasi in cui comprare un verbo scelto (usare un verbo comune) --> link to resource: https://sentence.yourdictionary.com/kill\n",
    "- Effettuare parsing e disambiguazione\n",
    "- Usare i supersensi di wordnet sugli argomenti (subj e obj nel caso di 2 argomenti) del verbo scelto\n",
    "- Calcolo risultati, frequenza e stampare cluster semantici ottenuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domande per il prof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il modo in ci etichettiamo \"person\" alle parole va bene?\n",
    "- Il modo in cui disambiguiamo, per quanto poco efficiente (lesk), è ok?\n",
    "- Va bene il modo in cui rappresentiamo i risultati (cluster semantici)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appunti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet supersense: Per ogni sinset abbiamo un supersenso associato (esempi su slide)\n",
    "Utilizzare i supersense dei synset per determinare il semantic type - nltk\n",
    "\n",
    "I supersense di wordenet non sono il massimo, vediamo alcune alternative:\n",
    " * CSI - Ai supersense ci sono associati delle categorie che possono essere utilizzati come supersense\n",
    "\n",
    "link to wordnet resource: https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\n",
    "\n",
    "*Spunti futuri:*\n",
    "\n",
    "**Come distinguo i synset?**\n",
    "--> Funzione lesk (https://www.nltk.org/howto/wsd.html#word-sense-disambiguation)\n",
    "\n",
    "**Come trovo il supersenso tra due termini?**\n",
    "--> Funzione Lowest Common Hypernyms (https://www.nltk.org/howto/wordnet_lch.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from nltk.wsd import lesk\n",
    "import re\n",
    "import spacy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* person: Usata per trovare il supersenso dei pronomi\n",
    "* patter: Usato per trovare il soggetto e l'oggetto del verbo kill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = [\"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"me\", \"him\", \"her\", \"his\", \"them\", \"someone\", \"us\", \"people\", \"anyone\"] \n",
    "\n",
    "pattern = [\n",
    "    {\"RIGHT_ID\": \"attr\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"kill\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"nsubj\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"dobj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"dobj\"]}}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### carico spacy e aggiungo il pattern al Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"pattern\", [pattern])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metodi utili per trovare il match e per la wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match( text):\n",
    "    # Find the pattern in the document\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = doc[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "        \n",
    "        return subj,dobj,phrase[0].tag_,phrase[len(phrase)-1].tag_\n",
    "    return \"\",\"\",\"\",\"\"\n",
    "\n",
    "def word_sense_disambiguation(list_words, word):\n",
    "    right_synset = lesk(list_words, word)\n",
    "    return right_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    res = text.split('.')\n",
    "    return res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "('artifact', 'communication'): 1.0%\n",
      "('person', 'group'): 1.5%\n",
      "('person', 'person'): 54.0%\n",
      "('competition', 'artifact'): 0.5%\n",
      "('person', 'communication'): 2.0%\n",
      "('food', 'person'): 0.5%\n",
      "('person', 'animal'): 3.0%\n",
      "('act', 'person'): 1.0%\n",
      "('person', 'all'): 4.0%\n",
      "('unknown', 'person'): 5.0%\n",
      "('person', 'artifact'): 2.0%\n",
      "('cognition', 'person'): 1.5%\n",
      "('person', 'unknown'): 5.0%\n",
      "('person', 'cognition'): 1.0%\n",
      "('unknown', 'animal'): 1.0%\n",
      "('body', 'person'): 0.5%\n",
      "('person', 'stative'): 0.5%\n",
      "('person', 'act'): 1.0%\n",
      "('group', 'person'): 3.0%\n",
      "('unknown', 'consumption'): 0.5%\n",
      "('communication', 'person'): 0.5%\n",
      "('contact', 'person'): 1.0%\n",
      "('person', 'emotion'): 1.0%\n",
      "('artifact', 'act'): 0.5%\n",
      "('act', 'cognition'): 0.5%\n",
      "('all', 'food'): 0.5%\n",
      "('unknown', 'plant'): 0.5%\n",
      "('communication', 'food'): 0.5%\n",
      "('artifact', 'all'): 0.5%\n",
      "('person', 'food'): 0.5%\n",
      "('motion', 'person'): 0.5%\n",
      "('animal', 'quantity'): 0.5%\n",
      "('social', 'person'): 1.0%\n",
      "('social', 'state'): 0.5%\n",
      "('person', 'change'): 0.5%\n",
      "('group', 'consumption'): 0.5%\n",
      "('Tops', 'person'): 0.5%\n",
      "('artifact', 'person'): 0.5%\n",
      "('food', 'time'): 0.5%\n",
      "('animal', 'artifact'): 0.5%\n"
     ]
    }
   ],
   "source": [
    "subj_ss = \"\"\n",
    "dobj_ss = \"\"\n",
    "struct = {}\n",
    "tot = 0\n",
    "with open ('../sentence_kill.txt', 'r', encoding=\"utf8\") as f:\n",
    "    for row in f:\n",
    "        subj_synset, dobj_synset, subj_ss, dobj_ss = None, None, \"\", \"\"\n",
    "        subj, dobj, stag, dtag = get_match(row)\n",
    "            \n",
    "        if subj != \"\" and dobj != \"\":\n",
    "            # Cerco il synset del soggetto e dell'oggetto e associo automaticamente il synset \"person\" se trovo\n",
    "            # un nome proprio o una sringa presente in person\n",
    "            \n",
    "            # Soggetto\n",
    "            if stag == \"NNP\" or subj.lower() in person:\n",
    "                subj_ss = \"person\"\n",
    "            else:\n",
    "                subj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), subj)\n",
    "            # Oggetto\n",
    "            if dtag == \"NNP\" or dobj.lower() in person:\n",
    "                dobj_ss = \"person\"\n",
    "            else:\n",
    "                dobj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), dobj)        \n",
    "                 \n",
    "            # Se subj_synset e' None, significa che abbiamo associato il synset person\n",
    "            if not subj_synset is None: \n",
    "                subj_ss = cleaner(subj_synset.lexname())\n",
    "            elif subj_ss != \"person\":\n",
    "                subj_ss = \"unknown\"\n",
    "            \n",
    "            # Soggetto\n",
    "            if not dobj_synset is None:\n",
    "                dobj_ss = cleaner(dobj_synset.lexname())    \n",
    "            elif dobj_ss != \"person\":\n",
    "                dobj_ss = \"unknown\"\n",
    "\n",
    "            # Oggetto\n",
    "            if (subj_ss, dobj_ss) in struct:\n",
    "                struct[(subj_ss, dobj_ss)] += 1\n",
    "            else:\n",
    "                struct[(subj_ss, dobj_ss)] = 1\n",
    "            \n",
    "for k, v in struct.items():\n",
    "    tot += v \n",
    "\n",
    "for k in struct.keys():\n",
    "    print(f\"{k}: {round(((struct[k]/tot)*100), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecchio metodo che non etichettava i nomi propri correttamente (per fare comparazione di performance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 236: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\andre\\Desktop\\Università\\Magistrale\\TLN\\PART 3\\tln-2022-third-part-lab\\es3_hanks\\hanks.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000018?line=15'>16</a>\u001b[0m struct \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000018?line=16'>17</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m (\u001b[39m'\u001b[39m\u001b[39m../sentence_kill.txt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000018?line=17'>18</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m f:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000018?line=18'>19</a>\u001b[0m         subj_ss, dobj_ss \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/andre/Desktop/Universit%C3%A0/Magistrale/TLN/PART%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000018?line=19'>20</a>\u001b[0m         subj, dobj \u001b[39m=\u001b[39m get_match_2(row)\n",
      "File \u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python310\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m codecs\u001b[39m.\u001b[39;49mcharmap_decode(\u001b[39minput\u001b[39;49m,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,decoding_table)[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 236: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "def get_match_2( text):\n",
    "    # Find the pattern in the document\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = doc[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "        \n",
    "        return subj,dobj\n",
    "    return \"\",\"\"\n",
    "\n",
    "subj_ss = \"\"\n",
    "dobj_ss = \"\"\n",
    "struct = {}\n",
    "with open ('../sentence_kill.txt', 'r') as f:\n",
    "    for row in f:\n",
    "        subj_ss, dobj_ss = \"\", \"\"\n",
    "        subj, dobj = get_match_2(row)\n",
    "        if subj != \"\" and dobj != \"\":\n",
    "            subj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), subj)\n",
    "            dobj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), dobj)\n",
    "            \n",
    "            if not subj_synset is None: \n",
    "                subj_ss = cleaner(subj_synset.lexname())\n",
    "            else:\n",
    "                if subj.lower() in person:\n",
    "                    subj_ss = \"person\"\n",
    "                else:\n",
    "                    subj_ss = subj.lower()\n",
    "            if not dobj_synset is None:\n",
    "                dobj_ss = cleaner(dobj_synset.lexname())    \n",
    "            else:\n",
    "                if dobj.lower() in person:\n",
    "                    dobj_ss = \"person\"\n",
    "                else:\n",
    "                    dobj_ss = dobj.lower()\n",
    "\n",
    "            if (subj_ss, dobj_ss) in struct:\n",
    "                struct[(subj_ss, dobj_ss)] += 1\n",
    "            else:\n",
    "                struct[(subj_ss, dobj_ss)] = 1\n",
    "            \n",
    "pprint(struct)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6847c98a8f86b01c6a19c518cd2f366693b80566b266804d5ca763cbb223f52b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
