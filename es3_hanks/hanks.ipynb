{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esercizio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consegna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scegliere un verbo transitivo --> **KILL**\n",
    "- Trovare un corpus con > 1000 frasi in cui comprare un verbo scelto (usare un verbo comune) --> link to resource: https://sentence.yourdictionary.com/kill\n",
    "- Effettuare parsing e disambiguazione\n",
    "- Usare i supersensi di wordnet sugli argomenti (subj e obj nel caso di 2 argomenti) del verbo scelto\n",
    "- Calcolo risultati, frequenza e stampare cluster semantici ottenuti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appunti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet supersense: Per ogni sinset abbiamo un supersenso associato (esempi su slide)\n",
    "Utilizzare i supersense dei synset per determinare il semantic type - nltk\n",
    "\n",
    "I supersense di wordenet non sono il massimo, vediamo alcune alternative:\n",
    " * CSI - Ai supersense ci sono associati delle categorie che possono essere utilizzati come supersense\n",
    "\n",
    "link to wordnet resource: https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\n",
    "\n",
    "*Spunti futuri:*\n",
    "\n",
    "**Come distinguo i synset?**\n",
    "--> Funzione lesk (https://www.nltk.org/howto/wsd.html#word-sense-disambiguation)\n",
    "\n",
    "**Come trovo il supersenso tra due termini?**\n",
    "--> Funzione Lowest Common Hypernyms (https://www.nltk.org/howto/wordnet_lch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from spacy.matcher import DependencyMatcher\n",
    "from nltk.wsd import lesk\n",
    "import re\n",
    "import spacy\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* person: Usata per trovare il supersenso dei pronomi\n",
    "* patter: Usato per trovare il soggetto e l'oggetto del verbo kill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = [\"i\", \"you\", \"he\", \"she\", \"we\", \"they\", \"me\", \"him\", \"her\", \"his\", \"them\", \"someone\"] \n",
    "\n",
    "pattern = [\n",
    "    {\"RIGHT_ID\": \"attr\",\n",
    "    \"RIGHT_ATTRS\": {\"LEMMA\": {\"IN\": [\"kill\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"subj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"nsubj\"]}}\n",
    "    },\n",
    "    {\"LEFT_ID\": \"attr\",\n",
    "    \"REL_OP\": \">\",\n",
    "    \"RIGHT_ID\": \"dobj\",\n",
    "    \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"dobj\"]}}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### carico spacy e aggiungo il pattern al Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "matcher.add(\"pattern\", [pattern])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metodi utili per trovare il match e per la wsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match( text):\n",
    "    # Find the pattern in the document\n",
    "    doc = nlp(text)\n",
    "    matches = matcher(doc)\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = doc[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "        \n",
    "        return subj,dobj\n",
    "    return \"\",\"\"\n",
    "\n",
    "def word_sense_disambiguation(list_words, word):\n",
    "    right_synset = lesk(list_words, word)\n",
    "    return right_synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "    res = text.split('.')\n",
    "    return res[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('Tops', 'animal'): 1,\n",
      " ('act', 'cognition'): 1,\n",
      " ('act', 'person'): 3,\n",
      " ('all', 'all'): 3,\n",
      " ('all', 'animal'): 2,\n",
      " ('all', 'artifact'): 1,\n",
      " ('all', 'change'): 1,\n",
      " ('all', 'cognition'): 1,\n",
      " ('all', 'communication'): 1,\n",
      " ('all', 'food'): 1,\n",
      " ('all', 'group'): 1,\n",
      " ('all', 'location'): 1,\n",
      " ('all', 'person'): 14,\n",
      " ('animal', 'Tops'): 1,\n",
      " ('animal', 'artifact'): 1,\n",
      " ('animal', 'himself'): 1,\n",
      " ('animal', 'location'): 1,\n",
      " ('animal', 'person'): 3,\n",
      " ('animal', 'quantity'): 1,\n",
      " ('artifact', 'act'): 1,\n",
      " ('artifact', 'all'): 1,\n",
      " ('artifact', 'communication'): 1,\n",
      " ('artifact', 'contact'): 1,\n",
      " ('artifact', 'person'): 1,\n",
      " ('attribute', 'cognition'): 1,\n",
      " ('body', 'person'): 1,\n",
      " ('cognition', 'group'): 1,\n",
      " ('cognition', 'location'): 1,\n",
      " ('cognition', 'person'): 3,\n",
      " ('communication', 'all'): 1,\n",
      " ('communication', 'food'): 1,\n",
      " ('communication', 'person'): 1,\n",
      " ('contact', 'person'): 1,\n",
      " ('czerno', 'location'): 1,\n",
      " ('food', 'person'): 1,\n",
      " ('food', 'time'): 1,\n",
      " ('group', 'consumption'): 1,\n",
      " ('group', 'person'): 5,\n",
      " ('group', 'stative'): 1,\n",
      " ('jonny', 'person'): 2,\n",
      " ('kenyahs', 'cognition'): 1,\n",
      " ('kris.ll', 'person'): 1,\n",
      " ('location', 'animal'): 1,\n",
      " ('motion', 'artifact'): 1,\n",
      " ('motion', 'communication'): 1,\n",
      " ('person', 'Tops'): 1,\n",
      " ('person', 'act'): 1,\n",
      " ('person', 'all'): 5,\n",
      " ('person', 'animal'): 9,\n",
      " ('person', 'anyone'): 2,\n",
      " ('person', 'artifact'): 2,\n",
      " ('person', 'communication'): 1,\n",
      " ('person', 'connor'): 1,\n",
      " ('person', 'czerno'): 1,\n",
      " ('person', 'emotion'): 3,\n",
      " ('person', 'group'): 1,\n",
      " ('person', 'jonny'): 1,\n",
      " ('person', 'location'): 5,\n",
      " ('person', 'person'): 30,\n",
      " ('person', 'state'): 2,\n",
      " ('person', 'stative'): 3,\n",
      " ('person', 'substance'): 2,\n",
      " ('person', 'those'): 1,\n",
      " ('person', 'yourself'): 1,\n",
      " ('quantity', 'all'): 1,\n",
      " ('quantity', 'artifact'): 1,\n",
      " ('quantity', 'myself'): 1,\n",
      " ('quantity', 'person'): 3,\n",
      " ('rhyn', 'state'): 1,\n",
      " ('shalt', 'person'): 1,\n",
      " ('sirian', 'person'): 1,\n",
      " ('social', 'location'): 2,\n",
      " ('social', 'state'): 1,\n",
      " ('substance', 'Tops'): 1,\n",
      " ('substance', 'all'): 1,\n",
      " ('substance', 'animal'): 1,\n",
      " ('substance', 'artifact'): 1,\n",
      " ('substance', 'cognition'): 1,\n",
      " ('substance', 'communication'): 2,\n",
      " ('substance', 'damian'): 1,\n",
      " ('substance', 'food'): 1,\n",
      " ('substance', 'himself'): 1,\n",
      " ('substance', 'location'): 3,\n",
      " ('substance', 'marius'): 1,\n",
      " ('substance', 'others'): 3,\n",
      " ('substance', 'person'): 18,\n",
      " ('substance', 'state'): 1,\n",
      " ('substance', 'those'): 1,\n",
      " ('taran', 'person'): 1,\n",
      " ('that', 'anyone'): 1,\n",
      " ('that', 'person'): 1,\n",
      " ('that', 'plant'): 1,\n",
      " ('that', 'stative'): 1,\n",
      " ('this', 'person'): 1,\n",
      " ('what', 'act'): 1,\n",
      " ('what', 'animal'): 2,\n",
      " ('what', 'consumption'): 1,\n",
      " ('what', 'person'): 2,\n",
      " ('which', 'all'): 1,\n",
      " ('which', 'animal'): 1,\n",
      " ('which', 'body'): 1,\n",
      " ('which', 'person'): 1,\n",
      " ('which', 'substance'): 1}\n"
     ]
    }
   ],
   "source": [
    "subj_ss = \"\"\n",
    "dobj_ss = \"\"\n",
    "struct = {}\n",
    "with open ('../sentence_kill.txt', 'r') as f:\n",
    "    for row in f:\n",
    "        subj_ss, dobj_ss = \"\", \"\"\n",
    "        subj, dobj = get_match(row)\n",
    "        if subj != \"\" and dobj != \"\":\n",
    "            subj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), subj)\n",
    "            dobj_synset = word_sense_disambiguation(re.findall(r'\\w+', row), dobj)\n",
    "            \n",
    "            if not subj_synset is None: \n",
    "                subj_ss = cleaner(subj_synset.lexname())\n",
    "            else:\n",
    "                if subj.lower() in person:\n",
    "                    subj_ss = \"person\"\n",
    "                else:\n",
    "                    subj_ss = subj.lower()\n",
    "            if not dobj_synset is None:\n",
    "                dobj_ss = cleaner(dobj_synset.lexname())    \n",
    "            else:\n",
    "                if dobj.lower() in person:\n",
    "                    dobj_ss = \"person\"\n",
    "                else:\n",
    "                    dobj_ss = dobj.lower()\n",
    "\n",
    "            if (subj_ss, dobj_ss) in struct:\n",
    "                struct[(subj_ss, dobj_ss)] += 1\n",
    "            else:\n",
    "                struct[(subj_ss, dobj_ss)] = 1\n",
    "            \n",
    "pprint(struct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test \n",
    "Utilizzato per codice di prova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metodo che cerca un match con il pattern riportato sopra. \n",
    "Il match che trova è del tipo (\"soggetto kill oggetto\"), tramite l'accesso alla prima e all'ultima parola riusciamo ad isolare soggetto e oggetto.\n",
    "Controlliamo se uno dei due (o entrambi) sono presenti nella lista sopra dichiarata, nel caso lo siano, sappiamo già che il loro \"supersense\" (o iperonimo, dobbiamo capire) è Person.\n",
    "\n",
    "Due problematiche:\n",
    "- nel caso in cui abbiamo più di un synset per ogni parola dobbiamo disambiguare e capire quale prendere.\n",
    "- nel caso ci siano altre parole (Others, nomi di persona,...) che non hanno un synset, dobbiamo capire come gestirli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match(name_pattern, pattern, text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    matched_elements = [] \n",
    "    matcher = DependencyMatcher(nlp.vocab)\n",
    "    matcher.add(name_pattern, [pattern])\n",
    "    doc = nlp(text.lower())\n",
    "    matches = matcher(doc)\n",
    "    matches.sort(key = lambda x : x[1])\n",
    "    for match in matches:\n",
    "        match_words = sorted(match[1])\n",
    "        phrase = doc[match_words[0]:match_words[len(match_words)-1]+1]\n",
    "\n",
    "        subj = phrase[0].text\n",
    "        dobj = phrase[len(phrase)-1].text\n",
    "\n",
    "\n",
    "        super_subj, super_dobj = \"\", \"\"\n",
    "        if subj in person:\n",
    "            super_subj = \"Person\"\n",
    "        elif dobj in person:\n",
    "            super_dobj = \"Person\"\n",
    "        \n",
    "        syn_subj = wordnet.synsets(subj)\n",
    "        syn_dobj = wordnet.synsets(dobj)\n",
    "        \n",
    "        if syn_subj == []:\n",
    "            print(f\"Frase: {text}\")\n",
    "            print(f\"Synset non trovato, il soggetto è {subj}\")\n",
    "        elif syn_dobj == []:\n",
    "            print(f\"Frase: {text}\")\n",
    "            print(f\"Synset non trovato, l'oggetto' è {dobj}\")\n",
    "        else:\n",
    "            syn_subj = syn_subj\n",
    "            syn_dobj = syn_dobj\n",
    "        '''\n",
    "        if super_subj != \"\":\n",
    "            print(f\"S:{subj}: {super_subj}\")\n",
    "        else:\n",
    "            print(f\"{subj}: {syn_subj}\")\n",
    "        \n",
    "        if super_dobj != \"\":\n",
    "            print(f\"S:{dobj}: {super_dobj}\")\n",
    "        else:\n",
    "            print(f\"{dobj}: {syn_dobj}\")\n",
    "        '''\n",
    "        #matched_elements.append(wordnet.synsets(subj)[0].supersense() ,wordnet.synsets(dobj)[0].supersense())\n",
    "    if matched_elements == []:\n",
    "        return False\n",
    "    return matched_elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD CODE AND NOTES:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci sono soggetti e oggetti che non hanno dei synset, ad esempio YOU non ha un wordnet synset, credo che si debbano eliminare dal dataset questi tipi di frase, perchè non si possono trovare gli iperonimi se non hai il synset di partenza. in alternativa, come ha detto il prof a lezione, si può mettere una regola per cui quando incontri uno di questi elementi metti 'person' come iperonimo di default\n",
    "\n",
    "IL DATASET HA UN BOTTO DI SOGGETTI E OGGETTI CON PRONOMI DI MERDA. UN PÒ BRUTTINO PER I NOSTRI SCOPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../sentence_cook.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/andrea/Desktop/Università/TLN/Parte 3/tln-2022-third-part-lab/es3_hanks/hanks.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/Universit%C3%A0/TLN/Parte%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000023?line=0'>1</a>\u001b[0m dataset \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/andrea/Desktop/Universit%C3%A0/TLN/Parte%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000023?line=1'>2</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m (\u001b[39m'\u001b[39;49m\u001b[39m../sentence_cook.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/Universit%C3%A0/TLN/Parte%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000023?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/andrea/Desktop/Universit%C3%A0/TLN/Parte%203/tln-2022-third-part-lab/es3_hanks/hanks.ipynb#ch0000023?line=3'>4</a>\u001b[0m         result \u001b[39m=\u001b[39m get_match(\u001b[39m\"\u001b[39m\u001b[39mpattern\u001b[39m\u001b[39m\"\u001b[39m, pattern, row)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../sentence_cook.txt'"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "with open ('../sentence_cook.txt', 'r') as f:\n",
    "    for row in f:\n",
    "        result = get_match(\"pattern\", pattern, row)\n",
    "        if result != False:\n",
    "            dataset.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cose utili sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('monkey.n.01'), Synset('imp.n.02'), Synset('tamper.v.01'), Synset('putter.v.02')]\n",
      "any of various long-tailed primates (excluding the prosimians)\n",
      "[Synset('new_world_monkey.n.01'), Synset('old_world_monkey.n.01')]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Synset('object.n.01'), Synset('organism.n.01'), Synset('physical_entity.n.01'), Synset('living_thing.n.01'), Synset('entity.n.01'), Synset('whole.n.02')]\n",
      "[Synset('entity.n.01')]\n",
      "antenato più vicino tra man e baby: [Synset('person.n.01')]\n"
     ]
    }
   ],
   "source": [
    "#print(f\"synsets di subj: {wordnet.synsets(subj)}; synsets di obj: {wordnet.synsets(obj)}\\n\\n\")\n",
    "#print(f\"iperonimo di subj: {wordnet.synsets(subj)[0].hypernyms()}, iperonimo di obj: {wordnet.synsets(obj)[0].hypernyms()}\\n\\n\")\n",
    "        \n",
    "print(wordnet.synsets(\"monkey\"))\n",
    "#print(wordnet.synsets(\"victor\")[0].supersense())\n",
    "print(wordnet.synsets(\"monkey\")[0].definition())\n",
    "print(wordnet.synsets(\"monkey\")[0].hyponyms())\n",
    "print(\"\\n\\n\\n\")\n",
    "print(wordnet.synsets(\"banana\")[0].common_hypernyms(wordnet.synsets(\"monkey\")[0]))\n",
    "print(wordnet.synsets(\"dog\")[0].root_hypernyms())\n",
    "print(f\"antenato più vicino tra man e baby: {wordnet.synsets('man')[0].lowest_common_hypernyms(wordnet.synsets('baby')[0])}\") # UTILE UTILE UTILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('monkey.n.01'), Synset('imp.n.02'), Synset('tamper.v.01'), Synset('putter.v.02')]\n",
      "any of various long-tailed primates (excluding the prosimians)\n",
      "noun.animal\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[Synset('physical_entity.n.01'), Synset('living_thing.n.01'), Synset('object.n.01'), Synset('entity.n.01'), Synset('whole.n.02'), Synset('organism.n.01')]\n",
      "[Synset('entity.n.01')]\n",
      "antenato più vicino tra man e baby: [Synset('person.n.01')]\n"
     ]
    }
   ],
   "source": [
    "#print(f\"synsets di subj: {wordnet.synsets(subj)}; synsets di obj: {wordnet.synsets(obj)}\\n\\n\")\n",
    "#print(f\"iperonimo di subj: {wordnet.synsets(subj)[0].hypernyms()}, iperonimo di obj: {wordnet.synsets(obj)[0].hypernyms()}\\n\\n\")\n",
    "        \n",
    "print(wordnet.synsets(\"monkey\"))\n",
    "print(wordnet.synsets(\"monkey\")[0].definition())\n",
    "print(wordnet.synsets(\"monkey\")[0].lexname())\n",
    "print(\"\\n\\n\\n\")\n",
    "print(wordnet.synsets(\"banana\")[0].common_hypernyms(wordnet.synsets(\"monkey\")[0]))\n",
    "print(wordnet.synsets(\"dog\")[0].root_hypernyms())\n",
    "print(f\"antenato più vicino tra man e baby: {wordnet.synsets('man')[0].lowest_common_hypernyms(wordnet.synsets('baby')[0])}\") # UTILE UTILE UTILE"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
