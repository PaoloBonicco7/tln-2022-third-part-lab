{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approccio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prendo il termine più frequente nelle definizioni, sarà il genus\n",
    "- Stopwords removing e lemming delle definizioni\n",
    "- Prelevo tutto il sottoalbero di hyponimi del genus\n",
    "- Prendo le definizioni (glossa) dei synset di cui ho trovato i hyponimi\n",
    "- Faccio confronto tra definizioni di wordnet e lista di definizioni\n",
    "- Restituisco il synset che ha definizioni più simile a quella della lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(path):\n",
    "    '''\n",
    "    Read a file and, after revoving all stopwords, return a list of words.\n",
    "    '''\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def get_most_freq_words(text, nword):\n",
    "    '''\n",
    "    Given a list of sententeces, return the nword most frequent words\n",
    "    in each row of the document.\n",
    "    '''\n",
    "    genus = []\n",
    "    for row in text:\n",
    "        c = Counter()\n",
    "        c.update(row)\n",
    "        genus.append(c.most_common(nword))\n",
    "    return genus\n",
    "\n",
    "def get_hypos(word):\n",
    "    '''\n",
    "    Return all the hyponyms of a word.\n",
    "    '''\n",
    "    syn = get_synset(word)\n",
    "    hypo_list = []\n",
    "    if(syn is not None):\n",
    "        hypo_list = list(set([w for s in syn.closure(lambda s:s.hyponyms()) for w in s.lemma_names()]))\n",
    "    return hypo_list\n",
    "\n",
    "def get_hypers(word):\n",
    "    '''\n",
    "    Return all the hypernyms of a word.\n",
    "    '''\n",
    "    syn = get_synset(word)\n",
    "    return syn.hypernyms()\n",
    "\n",
    "def get_synset(word):\n",
    "    '''\n",
    "    Retrurn the first synset of a word.\n",
    "    '''\n",
    "    if(len(wn.synsets(word)) > 0):\n",
    "        return wn.synsets(word)[0]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genus = 3 # number of most frequent words to search in the definitions file, used to determine genus\n",
    "num_most_freq_word = 10 # number of most frequent words to search in the definitions, used to compare with the wordnet's definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing data and find the genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('feeling', 11), ('human', 8), ('feel', 8)], [('human', 26), ('person', 5), ('homo', 5)], [('someone', 14), ('feeling', 7), ('anger', 7)], [('used', 22), ('object', 15), ('material', 13)]]\n"
     ]
    }
   ],
   "source": [
    "file = get_text_from_file('../res/def.csv')\n",
    "\n",
    "genus = get_most_freq_words(file, num_genus)\n",
    "\n",
    "print (genus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proviamo con 3 genus per aumentare accuratezza:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['feeling', 'human', 'feel'],\n",
       " ['human', 'person', 'homo'],\n",
       " ['someone', 'feeling', 'anger'],\n",
       " ['used', 'object', 'material']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genus_list = []\n",
    "for el in genus:\n",
    "    genus_list_inner = []\n",
    "    for el2 in el:\n",
    "        genus_list_inner.append(el2[0])\n",
    "    genus_list.append(genus_list_inner)\n",
    "        \n",
    "genus_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ['feeling', 'human', 'feel']\n",
      "Word: *appetence*, score: *3*\n",
      "Word: *unpleasantness*, score: *3*\n",
      "Word: *sentiment*, score: *3*\n",
      "Word: *reverence*, score: *3*\n",
      "Word: *sensitiveness*, score: *3*\n",
      "\t ['human', 'person', 'homo']\n",
      "Word: *Mordva*, score: *2*\n",
      "Word: *Onondaga*, score: *2*\n",
      "Word: *Maidu*, score: *2*\n",
      "Word: *Mari*, score: *2*\n",
      "Word: *Atakapa*, score: *2*\n",
      "\t ['someone', 'feeling', 'anger']\n",
      "Word: *sounding_board*, score: *5*\n",
      "Word: *stolidity*, score: *4*\n",
      "Word: *impassiveness*, score: *4*\n",
      "Word: *hate*, score: *4*\n",
      "Word: *hatred*, score: *4*\n",
      "\t ['used', 'object', 'material']\n",
      "Word: *brick*, score: *5*\n",
      "Word: *building_material*, score: *4*\n",
      "Word: *china_clay*, score: *3*\n",
      "Word: *porcelain_clay*, score: *3*\n",
      "Word: *china_stone*, score: *3*\n"
     ]
    }
   ],
   "source": [
    "# Extract the most used word in the definitions\n",
    "key_words_defs = get_most_freq_words(file, num_most_freq_word)\n",
    "\n",
    "for i in range(len(genus_list)):\n",
    "    \n",
    "    # Top 10 word used in the definitions\n",
    "    key_row = []\n",
    "    for el in key_words_defs[i]:\n",
    "        key_row.append(el[0])\n",
    "    \n",
    "    #! Version with 1 genun\n",
    "    # Get the hyponyms of the genus and find the definition of the hyponyms\n",
    "    # hypo_list = get_hypos(genus_list[i])\n",
    "    # print(hypo_list)\n",
    "    # hypo_def = []\n",
    "    # for hypo in hypo_list:\n",
    "    #     hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "    \n",
    "    #! version with multiple genus\n",
    "    hypo_list, hypo_def = [], []\n",
    "    for el in genus_list[i]:\n",
    "        hypo_list.append(get_hypos(el))\n",
    "        \n",
    "    hypo_list = [x for xs in hypo_list for x in xs]\n",
    "    for hypo in hypo_list:\n",
    "        hypo_def.append((hypo, get_synset(hypo).definition()))\n",
    "    \n",
    "    # Compare the definition of our definitions (def.csv file) with the definition of the hyponyms\n",
    "    res = []\n",
    "    for wndef in hypo_def: # Definition of the hyponyms in wordnet\n",
    "        score = 0\n",
    "        imp_words = []\n",
    "        for key_word in key_row: # Definition given by us\n",
    "            if(key_word in wndef[1]):\n",
    "                score += 1\n",
    "                imp_words.append(key_word)      \n",
    "        \n",
    "        # Store all the value\n",
    "        res.append((score, wndef[0], imp_words, wndef[1]))\n",
    "        \n",
    "    sorted_list = sorted(res, key=lambda x: x[0])\n",
    "    sorted_res = list(reversed(sorted_list))\n",
    "    print(\"\\t\",genus_list[i])\n",
    "    for k in range(min(len(sorted_res), 5)):\n",
    "        #* Long print\n",
    "        # print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*, the key words are *{sorted_res[k][2]}* and the definition is *{sorted_res[k][3]}*')\n",
    "        print(f'Word: *{sorted_res[k][1]}*, score: *{sorted_res[k][0]}*')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
