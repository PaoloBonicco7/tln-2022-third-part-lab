{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate file to store in a dataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.731667258175464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for list in lists_of_defs:\\n    no_stop = []\\n    no_stop_defs = {}\\n    for definition in list:\\n        # print(definition)\\n        text_tokens = word_tokenize(definition)\\n        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\\n        no_stop.append(tokens_without_sw)\\n        # print(tokens_without_sw)\\n    bricks_without_sw = no_stop'"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pd.read_csv('../def.csv'))\n",
    "\n",
    "# Save the dataframe in different variables and remove the useless columns\n",
    "emotion_defs = df.loc[0,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# person_defs = df.loc[1,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# revenge_defs = df.loc[2,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# brick_def = df.loc[3,\"P1\":].dropna().replace(',','', regex=True)\n",
    "\n",
    "# lists_of_defs = [emotion_defs, person_defs, revenge_defs, brick_def]\n",
    "\n",
    "# List of definitions for emotion\n",
    "list_defs = []\n",
    "for definition in emotion_defs:\n",
    "    list_defs.append(definition)\n",
    "\n",
    "# print (list_defs)\n",
    "sim = 0\n",
    "for item in list_defs:\n",
    "    doc1 = nlp(item)\n",
    "    for item2 in list_defs[1:]:\n",
    "        doc2 = nlp(item2)\n",
    "        sim = sim + doc1.similarity(doc2)\n",
    "    \n",
    "sim = sim / (len(list_defs)*len(list_defs))\n",
    "print(sim)\n",
    "\n",
    "\n",
    "'''for list in lists_of_defs:\n",
    "    no_stop = []\n",
    "    no_stop_defs = {}\n",
    "    for definition in list:\n",
    "        # print(definition)\n",
    "        text_tokens = word_tokenize(definition)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        no_stop.append(tokens_without_sw)\n",
    "        # print(tokens_without_sw)\n",
    "    bricks_without_sw = no_stop'''\n",
    "    \n",
    "# emotion_defs:\n",
    "\n",
    "# for definition in emotion_defs:\n",
    "#         text_tokens = word_tokenize(definition)\n",
    "#         tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "#         no_stop.append(tokens_without_sw)\n",
    "#         # print(tokens_without_sw)\n",
    "\n",
    "# list_def_brick = ' '.join([str(item) for item in bricks_without_sw])\n",
    "\n",
    "# print(list_def_brick) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwrords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv('../def2.csv'))\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "for i in range (37):\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "print(df['no_stop_words'])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = df.apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the csv file\n",
    "df = pd.read_csv('../def.csv')\n",
    "\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['0'], row['1'])\n",
    "\n",
    "# 2 Fare stemming/lemming di ogni frase\n",
    "# 3 calcolare similarit√† tra tutte le definizioni di una singola categoria\n",
    "'''for line in f:\n",
    "    print(line)'''\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
