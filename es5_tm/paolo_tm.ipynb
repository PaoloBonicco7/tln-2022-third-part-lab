{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prova approccio \n",
    "(https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#20topicdistributionacrossdocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis.gensim_models  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/paolobonicco/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    0\n",
      "0   Stephen Curry plays almost exclusively at the ...\n",
      "1                            and 1.7 steals per game.\n",
      "2   He is a high-percentage free throw shooter, wi...\n",
      "3   Curry is the Warriors' all-time free-throw lea...\n",
      "4   Micheal Jordan has led the NBA in free throw p...\n",
      "5   Lebron James has been selected to seven All-NB...\n",
      "6   As a leader within the Warriors organization, ...\n",
      "7   Although capable of stealing the ball, having ...\n",
      "8   He is used more for his offense while his team...\n",
      "9   Curry's shooting ability ranges from scoring i...\n",
      "10  Using an unorthodox jump shot, he is able to g...\n",
      "11  Several tiger specimens were described and pro...\n",
      "12  The validity of several tiger subspecies was q...\n",
      "13  Most putative subspecies described in the 19th...\n",
      "14  Morphologically, tigers from different regions...\n",
      "15  Therefore, it was proposed to recognize only t...\n",
      "16  After two unsuccessful seasons, Barcelona were...\n",
      "17  Upon the latter's departure, Messi was given t...\n",
      "18  He signed a new contract in July with an annua...\n",
      "19  Ahead of the new season, a major concern remai...\n",
      "20  Messi aiming to shoot during the 2009 UEFA Cha...\n",
      "21  In his first uninterrupted campaign,he scored ...\n",
      "22  During his first season under Barcelona's new ...\n",
      "23            Messi played mainly on the right wing. \n",
      "24  During the Clásico on 2 May 2009, however, Mes...\n",
      "25  positioned as a centre-forward but dropping de...\n",
      "26  Messi set up his side's first goal and scored ...\n",
      "27  he played his first final since breaking into ...\n",
      "28  With 23 league goals from Messi that season, B...\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "# open and read topics.txt file with pandas\n",
    "df = pd.read_csv('../topics.txt', sep='\\t', header=None)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2131\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2140\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es5_tm/paolo_tm.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es5_tm/paolo_tm.ipynb#ch0000006?line=0'>1</a>\u001b[0m \u001b[39m# Convert to list\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es5_tm/paolo_tm.ipynb#ch0000006?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m df[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es5_tm/paolo_tm.ipynb#ch0000006?line=3'>4</a>\u001b[0m \u001b[39m# Remove Emails\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es5_tm/paolo_tm.ipynb#ch0000006?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m [re\u001b[39m.\u001b[39msub(\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS*@\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mS*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms?\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, sent) \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m data]\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3506\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/virtual-envs/tln-2022-third-part-lab/venvdicaro/lib/python3.10/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df[1].tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usiamo la funzione di gensin *simple_preprocess* per tokenizzare ogni frase in una lista di parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un *bigramma* è un insieme di due parole che occorrono speso insieme nel documento, un *trigramma* è un insieme di 3 parole. Aumentando/diminuendo la soglia otteniamo meno/più parole accorporate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp_posting_host', 'rac_wam_umd_edu', 'organization', 'university', 'of', 'maryland_college_park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front_bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']\n"
     ]
    }
   ],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definiamo in maniera più ordinata le funzioni che abbiamo utilizzato fino ad adesso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E utilizziamo i metodi precedentemente generati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp_poste', 'host', 'umd', 'organization', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling\n",
    "\n",
    "The two main inputs to the LDA topic model are the dictionary(id2word) and the corpus. Let’s create them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 5), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pretty print the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('addition', 1),\n",
       "  ('body', 1),\n",
       "  ('bricklin', 1),\n",
       "  ('bring', 1),\n",
       "  ('call', 1),\n",
       "  ('car', 5),\n",
       "  ('day', 1),\n",
       "  ('door', 2),\n",
       "  ('early', 1),\n",
       "  ('engine', 1),\n",
       "  ('enlighten', 1),\n",
       "  ('funky', 1),\n",
       "  ('history', 1),\n",
       "  ('host', 1),\n",
       "  ('info', 1),\n",
       "  ('know', 1),\n",
       "  ('late', 1),\n",
       "  ('lerxst', 1),\n",
       "  ('line', 1),\n",
       "  ('look', 2),\n",
       "  ('mail', 1),\n",
       "  ('make', 1),\n",
       "  ('model', 1),\n",
       "  ('name', 1),\n",
       "  ('neighborhood', 1),\n",
       "  ('nntp_poste', 1),\n",
       "  ('organization', 1),\n",
       "  ('park', 1),\n",
       "  ('production', 1),\n",
       "  ('really', 1),\n",
       "  ('rest', 1),\n",
       "  ('s', 1),\n",
       "  ('see', 1),\n",
       "  ('separate', 1),\n",
       "  ('small', 1),\n",
       "  ('spec', 1),\n",
       "  ('sport', 1),\n",
       "  ('tellme', 1),\n",
       "  ('thank', 1),\n",
       "  ('thing', 1),\n",
       "  ('umd', 1),\n",
       "  ('wonder', 1),\n",
       "  ('year', 1)]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything required to train the LDA model. In addition to the corpus and dictionary, you need to provide the number of topics as well.\n",
    "\n",
    "Apart from that, alpha and eta are hyperparameters that affect sparsity of the topics. According to the Gensim docs, both defaults to 1.0/num_topics prior.\n",
    "\n",
    "chunksize is the number of documents to be used in each training chunk. update_every determines how often the model parameters should be updated and passes is the total number of training passes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LDA model is built with 20 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic.\n",
    "\n",
    "You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.064*\"nhl\" + 0.059*\"recommend\" + 0.051*\"gateway\" + 0.039*\"flight\" + '\n",
      "  '0.031*\"fuel\" + 0.027*\"floor\" + 0.024*\"bank\" + 0.018*\"space_station\" + '\n",
      "  '0.017*\"phase\" + 0.017*\"qualified\"'),\n",
      " (1,\n",
      "  '0.095*\"key\" + 0.040*\"physical\" + 0.037*\"public\" + 0.028*\"encryption\" + '\n",
      "  '0.027*\"chip\" + 0.025*\"security\" + 0.022*\"private\" + 0.021*\"master\" + '\n",
      "  '0.020*\"government\" + 0.018*\"clipper\"'),\n",
      " (2,\n",
      "  '0.028*\"believe\" + 0.025*\"evidence\" + 0.023*\"reason\" + 0.018*\"say\" + '\n",
      "  '0.017*\"claim\" + 0.015*\"christian\" + 0.015*\"sense\" + 0.013*\"exist\" + '\n",
      "  '0.012*\"fact\" + 0.012*\"faith\"'),\n",
      " (3,\n",
      "  '0.072*\"team\" + 0.069*\"game\" + 0.050*\"play\" + 0.048*\"win\" + 0.040*\"year\" + '\n",
      "  '0.034*\"player\" + 0.024*\"season\" + 0.018*\"fan\" + 0.018*\"goal\" + 0.017*\"run\"'),\n",
      " (4,\n",
      "  '0.093*\"ide\" + 0.078*\"mother\" + 0.040*\"remind\" + 0.019*\"ultimate\" + '\n",
      "  '0.015*\"winter\" + 0.012*\"beauty\" + 0.011*\"absurd\" + 0.009*\"grip\" + '\n",
      "  '0.004*\"credibility\" + 0.002*\"stall\"'),\n",
      " (5,\n",
      "  '0.135*\"monitor\" + 0.043*\"rd\" + 0.034*\"trivial\" + 0.021*\"suck\" + '\n",
      "  '0.009*\"space_dig\" + 0.009*\"added_forwarde\" + 0.008*\"rod\" + 0.004*\"infinity\" '\n",
      "  '+ 0.004*\"caution\" + 0.003*\"golden\"'),\n",
      " (6,\n",
      "  '0.033*\"government\" + 0.032*\"state\" + 0.020*\"country\" + 0.017*\"attack\" + '\n",
      "  '0.016*\"war\" + 0.015*\"israeli\" + 0.013*\"city\" + 0.013*\"greek\" + '\n",
      "  '0.013*\"force\" + 0.013*\"soldier\"'),\n",
      " (7,\n",
      "  '0.057*\"people\" + 0.023*\"group\" + 0.020*\"man\" + 0.020*\"say\" + 0.018*\"book\" + '\n",
      "  '0.016*\"issue\" + 0.014*\"live\" + 0.013*\"name\" + 0.012*\"day\" + 0.011*\"person\"'),\n",
      " (8,\n",
      "  '0.021*\"get\" + 0.016*\"go\" + 0.016*\"make\" + 0.015*\"write\" + 0.015*\"know\" + '\n",
      "  '0.014*\"think\" + 0.013*\"good\" + 0.013*\"time\" + 0.012*\"well\" + 0.012*\"see\"'),\n",
      " (9,\n",
      "  '0.135*\"line\" + 0.082*\"organization\" + 0.061*\"nntp_poste\" + 0.059*\"write\" + '\n",
      "  '0.055*\"host\" + 0.051*\"article\" + 0.028*\"thank\" + 0.025*\"reply\" + '\n",
      "  '0.022*\"university\" + 0.017*\"card\"'),\n",
      " (10,\n",
      "  '0.128*\"graphic\" + 0.067*\"mount\" + 0.054*\"convert\" + 0.042*\"workstation\" + '\n",
      "  '0.030*\"capture\" + 0.024*\"please_respond\" + 0.021*\"camera\" + '\n",
      "  '0.018*\"positively\" + 0.011*\"creature\" + 0.009*\"weeks_ago\"'),\n",
      " (11,\n",
      "  '0.094*\"child\" + 0.049*\"fire\" + 0.048*\"drug\" + 0.031*\"kid\" + '\n",
      "  '0.030*\"corporation\" + 0.026*\"die\" + 0.025*\"trial\" + 0.025*\"firearm\" + '\n",
      "  '0.024*\"boy\" + 0.024*\"wife\"'),\n",
      " (12,\n",
      "  '0.099*\"law\" + 0.075*\"gun\" + 0.029*\"crime\" + 0.027*\"weapon\" + 0.026*\"murder\" '\n",
      "  '+ 0.025*\"revelation\" + 0.020*\"blank\" + 0.017*\"death\" + 0.017*\"hole\" + '\n",
      "  '0.017*\"vote\"'),\n",
      " (13,\n",
      "  '0.107*\"space\" + 0.068*\"science\" + 0.040*\"field\" + 0.038*\"earth\" + '\n",
      "  '0.030*\"launch\" + 0.028*\"scientific\" + 0.026*\"orbit\" + 0.025*\"mission\" + '\n",
      "  '0.025*\"moon\" + 0.021*\"satellite\"'),\n",
      " (14,\n",
      "  '0.062*\"armenian\" + 0.048*\"turk\" + 0.042*\"turkish\" + 0.041*\"muslim\" + '\n",
      "  '0.021*\"islamic\" + 0.020*\"escape\" + 0.018*\"relation\" + 0.017*\"proceed\" + '\n",
      "  '0.015*\"genocide\" + 0.015*\"turkey\"'),\n",
      " (15,\n",
      "  '0.070*\"cop\" + 0.065*\"clipper_chip\" + 0.048*\"proposal\" + 0.040*\"crypto\" + '\n",
      "  '0.040*\"export\" + 0.021*\"revolver\" + 0.019*\"police\" + 0.015*\"entitle\" + '\n",
      "  '0.009*\"radar\" + 0.006*\"privately\"'),\n",
      " (16,\n",
      "  '0.124*\"image\" + 0.062*\"format\" + 0.062*\"scan\" + 0.056*\"family\" + '\n",
      "  '0.049*\"scsi\" + 0.041*\"headache\" + 0.023*\"quadra\" + 0.021*\"intel\" + '\n",
      "  '0.020*\"utility\" + 0.019*\"specification\"'),\n",
      " (17,\n",
      "  '0.776*\"ax\" + 0.019*\"wing\" + 0.016*\"direct\" + 0.009*\"dual\" + 0.008*\"quick\" + '\n",
      "  '0.007*\"trace\" + 0.006*\"human_being\" + 0.004*\"partner\" + 0.004*\"dirty\" + '\n",
      "  '0.004*\"quran\"'),\n",
      " (18,\n",
      "  '0.048*\"drive\" + 0.031*\"car\" + 0.022*\"buy\" + 0.021*\"high\" + 0.020*\"power\" + '\n",
      "  '0.020*\"price\" + 0.018*\"sell\" + 0.018*\"sale\" + 0.018*\"cost\" + 0.016*\"low\"'),\n",
      " (19,\n",
      "  '0.027*\"system\" + 0.024*\"use\" + 0.018*\"program\" + 0.017*\"file\" + '\n",
      "  '0.014*\"window\" + 0.013*\"run\" + 0.012*\"include\" + 0.011*\"available\" + '\n",
      "  '0.011*\"information\" + 0.010*\"source\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the model performe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  -13.368807304445435\n",
      "\n",
      "Coherence Score:  0.5237564492819576\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
