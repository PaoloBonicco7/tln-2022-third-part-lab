{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzionamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a calcolare la frequenza delle parole più utilizzate nel documento. La somma delle frequenze delle parole più utilizzate rappresentarà il\n",
    "punteggio assegnato ad ogni segmento. lo scopo dell'algoritmo è trovare i tagli in modo da massimizzare la somma del punteggio.\n",
    "Nella prima implementazione l'algoritmo ha bisogno del numero di tagli in ingresso.\n",
    "\n",
    "- **COOCCORENCE** : Indica la frequenza delle parole più utilizzate nel segmento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset è costituito da pezzi dei paragrafi di wikipedia di 3 argomenti diversi, in questo caso i 3 argomenti sono:\n",
    "- Gorillas\n",
    "- Quantum Computing\n",
    "- Astronomy\n",
    "\n",
    "I tagli corretti sono alla riga 59/60 e alla riga 102/103."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_ita(phrase):\n",
    "    stop_words = stopwords.words('italian')\n",
    "    phrase = phrase.split()\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    return phrase\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def cooccurrence(text, n_most_words):\n",
    "    '''\n",
    "    Calculates the cooccurrence value of a sequence of words. \n",
    "    This value correspond to the sum of the occurrences of the n most frequently used words in the word list.\n",
    "    '''\n",
    "    score = 0\n",
    "    c = Counter()\n",
    "    most_common = []\n",
    "    for row in text:\n",
    "        c.update(row)\n",
    "        \n",
    "    most_common = c.most_common(n_most_words)\n",
    "    # print(most_common)\n",
    "    \n",
    "    for el in most_common:\n",
    "        score = score + el[1]\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_segment(file, start, end):\n",
    "    '''\n",
    "    Given the first and the last line, extract the segment.\n",
    "    '''\n",
    "    segment = []\n",
    "    for i in range(start, end):\n",
    "        segment.append(file[i])\n",
    "    return segment\n",
    "\n",
    "def get_cuts(ntopic):\n",
    "    '''\n",
    "    Generate a list of random value between 1 and the number of lines in the documents,\n",
    "    that correspond to the cuts of the documents. The first value is 0 and the last is\n",
    "    the numebers of lines in the documents.\n",
    "    '''\n",
    "    cut = [0] * ntopic\n",
    "    while not(all(cut[i] < cut[i+1] for i in range(len(cut) - 1))):\n",
    "        for k in range(1, ntopic): # generate the cuts\n",
    "            cut[k] = random.randint(1, num_lines-1)\n",
    "    cut.append(num_lines)\n",
    "        \n",
    "    return cut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dopo aver ripulito il testo, rimuovendo le stopword, organizziamo le parole in una lista costituita da liste di parole, \n",
    "ogni lista contiene le parole tokenizzate di una linea del testo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quantum', 63), ('gorilla', 34), ('gorillas', 31), ('astronomy', 25), ('also', 17), ('classical', 17), ('algorithm', 16), ('stars', 16), ('silverback', 15), ('astronomical', 15), ('ray', 15), ('females', 14), ('computers', 14), ('computer', 14), ('wavelengths', 14), ('males', 13), ('known', 13), ('model', 13), ('algorithms', 13), ('early', 13), ('earth', 13), ('light', 13), ('infrared', 13), ('male', 12), ('problems', 12), ('planets', 12), ('western', 11), ('ft', 11), ('may', 11), ('made', 11)]\n"
     ]
    }
   ],
   "source": [
    "file = get_text_from_file('../res/segmentation_eng.txt')\n",
    "c = Counter()\n",
    "num_lines = sum(1 for line in open('../res/segmentation_eng.txt')) # Number of lines in the file\n",
    "\n",
    "for row in file:\n",
    "    c.update(row)\n",
    "    \n",
    "print(c.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "45\n",
      "[7, 64, 105]\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "cut = [0 , 3, 45, num_lines]\n",
    "# Extract the segments\n",
    "for i in range(len(cut)-1):\n",
    "    print(cut[i])\n",
    "    text = extract_segment(file, cut[i], cut[i+1])\n",
    "    scores.append(cooccurrence(text))\n",
    "    \n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters - no more used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x , y, max = 0, 0, 0 # x = cut1, y = cut2, max = max cooccurrence value\n",
    "# max_scores = [(0, 0), (0, 0), (0, 0)] # Used to store the value of a segment with the highest cooccurrence value\n",
    "# max_cut = [] # Used to store the cuts of the segments with the highest cooccurrence value\n",
    "# scores = [0, 0, 0] # Inizialize the array to store the cooccurrence value of each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scores(file, ntopic, n_most_words): # n_most_words = number of most used words, used in cooccurrence()\n",
    "    scores = [0]*ntopic\n",
    "    sum_scores, max = 0, 0\n",
    "    max_cut = []\n",
    "    \n",
    "    for f in range(2000): \n",
    "        # First generate the random cuts\n",
    "        cut = get_cuts(ntopic)\n",
    "        \n",
    "        # Extract the segments by the cuts and calculate the cooccurrence value for each segment\n",
    "        # based on the random cuts\n",
    "        for i in range(len(cut)-1):\n",
    "            text = extract_segment(file, cut[i], cut[i+1])\n",
    "            scores[i] = cooccurrence(text, n_most_words)\n",
    "        \n",
    "        # Evaluate the result and store the best result\n",
    "        sum_scores = sum(scores)\n",
    "        if(sum_scores > max):\n",
    "            max = sum_scores\n",
    "            max_cut = cut\n",
    "            \n",
    "    return max_cut, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Best cut at lines \n",
      "[0, 54, 102, 181]\n",
      " the max sum is \n",
      "229\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_most_words = 3\n",
    "n_topics = 3\n",
    "n_iteration = 2000 # Number of iteration to generate the best result - higher value = more time and better result\n",
    "\n",
    "res = max_scores(file, n_topics, n_most_words)\n",
    "\n",
    "print(f'\\n Best cut at lines \\n{res[0]}\\n the max sum is \\n{res[1]}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a Try - Segmentation without number of topics\n",
    "## Non funziona e in ogni caso impiega troppo tempo ad eseguire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max is 300 at [0, 17, 54, 81, 100, 120, 158, 173, 181] with 8 topics\n"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "max_cut = []\n",
    "max_n_topic = 0\n",
    "\n",
    "for f in range(2):\n",
    "    ntopic = random.randint(2, 10)\n",
    "    res = max_scores(file, ntopic)\n",
    "    \n",
    "    if(max < res[1]):\n",
    "        max = res[1]\n",
    "        max_cut = res[0]\n",
    "        max_n_topic = ntopic\n",
    "        \n",
    "print(f'max is {max} at {max_cut} with {max_n_topic} topics')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
