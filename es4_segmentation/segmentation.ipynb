{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Esercizio 4 - Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funzionamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andiamo a calcolare la frequenza delle parole più utilizzate nel documento. La somma delle frequenze delle parole più utilizzate rappresentarà il\n",
    "punteggio assegnato ad ogni segmento. lo scopo dell'algoritmo è trovare i tagli in modo da massimizzare la somma del punteggio.\n",
    "Nella prima implementazione l'algoritmo ha bisogno del numero di tagli in ingresso.\n",
    "\n",
    "- **COOCCORENCE** : Indica la frequenza delle parole più utilizzate nel segmento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dataset è costituito da pezzi dei paragrafi di wikipedia di 3 argomenti diversi, in questo caso i 3 argomenti sono:\n",
    "- Gorillas\n",
    "- Quantum Computing\n",
    "- Astronomy\n",
    "- Kendrick Lamar Biography\n",
    "\n",
    "I tagli corretti sono alla riga 59/60, alla riga 102/103 e alla riga 181/182."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.test.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words_ita(phrase):\n",
    "    stop_words = stopwords.words('italian')\n",
    "    phrase = phrase.split()\n",
    "    phrase = [word for word in phrase if word not in stop_words]\n",
    "    return phrase\n",
    "\n",
    "def get_text_from_file(path):\n",
    "    file = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    with open (path, 'r') as f:\n",
    "        for row in f:\n",
    "            filtered_s = [w for w in word_tokenize(row) if not w.lower() in stop_words]\n",
    "            file.append(simple_preprocess(str(filtered_s), deacc=True))\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "def cooccurrence(text, n_most_words):\n",
    "    '''\n",
    "    Calculates the cooccurrence value of a sequence of words. \n",
    "    This value correspond to the sum of the occurrences of the n most frequently used words in the word list.\n",
    "    '''\n",
    "    score = 0\n",
    "    c = Counter()\n",
    "    most_common = []\n",
    "    for row in text:\n",
    "        c.update(row)\n",
    "        \n",
    "    most_common = c.most_common(n_most_words)\n",
    "    # print(most_common)\n",
    "    \n",
    "    for el in most_common:\n",
    "        score = score + el[1]\n",
    "        \n",
    "    return score\n",
    "\n",
    "def extract_segment(file, start, end):\n",
    "    '''\n",
    "    Given the first and the last line, extract the segment.\n",
    "    '''\n",
    "    segment = []\n",
    "    for i in range(start, end):\n",
    "        segment.append(file[i])\n",
    "    return segment\n",
    "\n",
    "def get_cuts(ntopic):\n",
    "    '''\n",
    "    Generate a list of random value between 1 and the number of lines in the documents,\n",
    "    that correspond to the cuts of the documents. The first value is 0 and the last is\n",
    "    the numebers of lines in the documents.\n",
    "    '''\n",
    "    cut = [0] * ntopic\n",
    "    while not(all(cut[i] < cut[i+1] for i in range(len(cut) - 1))):\n",
    "        for k in range(1, ntopic): # generate the cuts\n",
    "            cut[k] = random.randint(1, num_lines-1)\n",
    "    cut.append(num_lines)\n",
    "        \n",
    "    return cut\n",
    "\n",
    "def load_file(file_path, n_most_common):\n",
    "    file = get_text_from_file(file_path)\n",
    "    c = Counter()\n",
    "    num_lines = sum(1 for line in open(file_path)) # Number of lines in the file\n",
    "\n",
    "    for row in file:\n",
    "        c.update(row)\n",
    "        \n",
    "    return num_lines, file, c\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters - no more used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x , y, max = 0, 0, 0 # x = cut1, y = cut2, max = max cooccurrence value\n",
    "# max_scores = [(0, 0), (0, 0), (0, 0)] # Used to store the value of a segment with the highest cooccurrence value\n",
    "# max_cut = [] # Used to store the cuts of the segments with the highest cooccurrence value\n",
    "# scores = [0, 0, 0] # Inizialize the array to store the cooccurrence value of each segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_scores(file, ntopic, n_most_words, n_iteration): # n_most_words = number of most used words, used in cooccurrence()\n",
    "    scores = [0]*ntopic\n",
    "    sum_scores, max = 0, 0\n",
    "    max_cut = []\n",
    "    \n",
    "    for f in range(n_iteration): \n",
    "        # First generate the random cuts\n",
    "        cut = get_cuts(ntopic)\n",
    "        \n",
    "        # Extract the segments by the cuts and calculate the cooccurrence value for each segment\n",
    "        # based on the random cuts\n",
    "        for i in range(len(cut)-1):\n",
    "            text = extract_segment(file, cut[i], cut[i+1])\n",
    "            scores[i] = cooccurrence(text, n_most_words)\n",
    "        \n",
    "        # Evaluate the result and store the best result\n",
    "        sum_scores = sum(scores)\n",
    "        if(sum_scores > max):\n",
    "            max = sum_scores\n",
    "            max_cut = cut\n",
    "            \n",
    "    return max_cut, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lamar', 78), ('quantum', 63), ('gorilla', 34), ('gorillas', 31), ('released', 30), ('also', 27), ('astronomy', 25), ('album', 25), ('first', 24), ('dre', 19), ('classical', 18), ('mixtape', 17), ('algorithm', 16), ('stars', 16), ('silverback', 15), ('early', 15), ('astronomical', 15), ('ray', 15), ('song', 15), ('years', 14), ('females', 14), ('made', 14), ('new', 14), ('computers', 14), ('computer', 14), ('wavelengths', 14), ('video', 14), ('males', 13), ('known', 13), ('may', 13)]\n"
     ]
    }
   ],
   "source": [
    "n_most_words = 3\n",
    "n_topics = 4\n",
    "n_iteration = 40000 # Number of iteration to generate the best result - higher value = more time and better result\n",
    "c = Counter()\n",
    "\n",
    "file_data = load_file('../res/segmentation_eng.txt', 30) # Load the file and get the number of lines and the list of most common word in the file\n",
    "\n",
    "num_lines = file_data[0]\n",
    "file = file_data[1]\n",
    "\n",
    "for row in file:\n",
    "    c.update(row)\n",
    "    \n",
    "print(c.most_common(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best cut at lines -->  [0, 56, 102, 178, 254]\n",
      "the max sum is    -->  361\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res = max_scores(file, n_topics, n_most_words, n_iteration)\n",
    "\n",
    "print(f'\\nBest cut at lines -->  {res[0]}\\nthe max sum is    -->  {res[1]}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just a Try - Segmentation without number of topics\n",
    "## Non funziona e in ogni caso impiega troppo tempo ad eseguire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max_scores() missing 2 required positional arguments: 'n_most_words' and 'n_iteration'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb#ch0000031?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m2\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb#ch0000031?line=5'>6</a>\u001b[0m     ntopic \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m2\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb#ch0000031?line=6'>7</a>\u001b[0m     res \u001b[39m=\u001b[39m max_scores(file, ntopic)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb#ch0000031?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m(\u001b[39mmax\u001b[39m \u001b[39m<\u001b[39m res[\u001b[39m1\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/paolobonicco/virtual-envs/tln-2022-third-part-lab/es4_segmentation/segmentation.ipynb#ch0000031?line=9'>10</a>\u001b[0m         \u001b[39mmax\u001b[39m \u001b[39m=\u001b[39m res[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: max_scores() missing 2 required positional arguments: 'n_most_words' and 'n_iteration'"
     ]
    }
   ],
   "source": [
    "max = 0\n",
    "max_cut = []\n",
    "max_n_topic = 0\n",
    "\n",
    "for f in range(2):\n",
    "    ntopic = random.randint(2, 10)\n",
    "    res = max_scores(file, ntopic)\n",
    "    \n",
    "    if(max < res[1]):\n",
    "        max = res[1]\n",
    "        max_cut = res[0]\n",
    "        max_n_topic = ntopic\n",
    "        \n",
    "print(f'max is {max} at {max_cut} with {max_n_topic} topics')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
