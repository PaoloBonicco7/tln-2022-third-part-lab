{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipulate file to store in a dataFrame object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the data from csv\n",
    "df = pd.DataFrame(pd.read_csv('../def.csv'))\n",
    "\n",
    "# Create stemmer - porter works better than english\n",
    "stemmer2 = SnowballStemmer(\"porter\")\n",
    "\n",
    "# Save the dataframe in different variables and remove the useless columns\n",
    "emotion_defs = df.loc[0,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# person_defs = df.loc[1,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# revenge_defs = df.loc[2,\"P1\":].dropna().replace(',','', regex=True)\n",
    "# brick_def = df.loc[3,\"P1\":].dropna().replace(',','', regex=True)\n",
    "\n",
    "# lists_of_defs = [emotion_defs, person_defs, revenge_defs, brick_def]\n",
    "\n",
    "# Create a list of all the defs after stop word removing and stemming\n",
    "list_defs = []\n",
    "for definition in emotion_defs:\n",
    "    text_tokens = word_tokenize(definition) # ['Range', 'of', 'concepts', 'human', 'beings', 'feel', 'in', 'certain', 'situations']\n",
    "    \n",
    "    # remove stop words\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    \n",
    "    tokens_without_sw_stm = []\n",
    "    for word in tokens_without_sw:\n",
    "        word = stemmer2.stem(word)\n",
    "        tokens_without_sw_stm.append(word)\n",
    "    list_defs.append(tokens_without_sw_stm)    \n",
    "    \n",
    "print(list_defs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with the word2vec method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    from collections import Counter\n",
    "    from math import sqrt\n",
    "\n",
    "    # count the characters in word\n",
    "    cw = Counter(word)\n",
    "    # precomputes a set of the different characters\n",
    "    sw = set(cw)\n",
    "    # precomputes the \"length\" of the word vector\n",
    "    lw = sqrt(sum(c*c for c in cw.values()))\n",
    "\n",
    "    # return a tuple\n",
    "    return cw, sw, lw\n",
    "\n",
    "def cosdis(v1, v2):\n",
    "    # which characters are common to the two words?\n",
    "    common = v1[1].intersection(v2[1])\n",
    "    # by definition of cosine distance we have\n",
    "    return sum(v1[0][ch]*v2[0][ch] for ch in common)/v1[2]/v2[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_A = list_defs[0]\n",
    "list_B = list_defs[1]\n",
    "\n",
    "threshold = 0.80     # if needed\n",
    "for key in list_A:\n",
    "    for word in list_B:\n",
    "        try:\n",
    "            # print(key)\n",
    "            # print(word)\n",
    "            res = cosdis(word2vec(word), word2vec(key))\n",
    "            # print(res)\n",
    "            print(\"The cosine similarity between : {} and : {} is: {}\".format(word, key, res*100))\n",
    "            # if res > threshold:\n",
    "            #     print(\"Found a word with cosine distance > 80 : {} with original word: {}\".format(word, key))\n",
    "        except IndexError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'rang': 1, 'concept': 1, 'human': 1, 'be': 1, 'feel': 1, 'certain': 1, 'situat': 1})\n",
      "Counter({'someth': 1, 'feel': 1})\n",
      "26.726124191242434\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "counterA = Counter(list_A)\n",
    "counterB = Counter(list_B)\n",
    "\n",
    "\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n",
    "\n",
    "print(counterA)\n",
    "print(counterB)\n",
    "print(counter_cosine_similarity(counterA, counterB) * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity with wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity with wordnet\n",
    "for word in list_defs:\n",
    "    syn1 = wordnet.synsets('hello')[0]\n",
    "    syn2 = wordnet.synsets('selling')[0]\n",
    "    \n",
    "    print (\"hello name :  \", syn1.name())\n",
    "    print (\"selling name :  \", syn2.name())\n",
    "\n",
    "    print(syn1.wup_similarity(syn2))\n",
    "\n",
    "# First version of similarity - not working because the diveder at the end\n",
    "'''sim = 0\n",
    "for item in list_defs:\n",
    "    doc1 = nlp(item)\n",
    "    for item2 in list_defs[1:]:\n",
    "        doc2 = nlp(item2)\n",
    "        sim = sim + doc1.similarity(doc2)\n",
    "    \n",
    "sim = sim / (len(list_defs)*len(list_defs))\n",
    "print(sim)'''\n",
    "\n",
    "'''for list in lists_of_defs:\n",
    "    no_stop = []\n",
    "    no_stop_defs = {}\n",
    "    for definition in list:\n",
    "        # print(definition)\n",
    "        text_tokens = word_tokenize(definition)\n",
    "        tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        no_stop.append(tokens_without_sw)\n",
    "        # print(tokens_without_sw)\n",
    "    bricks_without_sw = no_stop'''\n",
    "    \n",
    "# emotion_defs:\n",
    "\n",
    "# for definition in emotion_defs:\n",
    "#         text_tokens = word_tokenize(definition)\n",
    "#         tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "#         no_stop.append(tokens_without_sw)\n",
    "#         # print(tokens_without_sw)\n",
    "\n",
    "# list_def_brick = ' '.join([str(item) for item in bricks_without_sw])\n",
    "\n",
    "# print(list_def_brick) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwrords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df = pd.DataFrame(pd.read_csv('../def2.csv'))\n",
    "\n",
    "# Exclude stopwords with Python's list comprehension and pandas.DataFrame.apply.\n",
    "for i in range (37):\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "\n",
    "print(df['no_stop_words'])\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "df = df.apply(lambda x: [item for item in x if item not in stop])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the csv file\n",
    "df = pd.read_csv('../def.csv')\n",
    "\n",
    "df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(row['0'], row['1'])\n",
    "\n",
    "# 2 Fare stemming/lemming di ogni frase\n",
    "# 3 calcolare similarit√† tra tutte le definizioni di una singola categoria\n",
    "'''for line in f:\n",
    "    print(line)'''\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3fc711e4ee2b6550a1e55fb58a23702f691a325dedbbea2e10fa55c056de4b98"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('venvdicaro': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
